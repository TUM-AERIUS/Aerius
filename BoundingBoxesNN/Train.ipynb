{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train CNN to predict bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import BB_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = tf.data.TFRecordDataset('train_with_negative.record')\n",
    "dataset_train = dataset_train.shuffle(buffer_size=5000)\n",
    "iterator_train = dataset_train.make_initializable_iterator()\n",
    "next_element_train = iterator_train.get_next()\n",
    "\n",
    "dataset_val = tf.data.TFRecordDataset('val_with_negative.record')\n",
    "dataset_val = dataset_val.shuffle(buffer_size=5000)\n",
    "iterator_val = dataset_val.make_initializable_iterator()\n",
    "next_element_val = iterator_val.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-10ba341faa24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_bb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vincent/Documents/TUM/DL4CV/Exercise/dl4cv/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vincent/Documents/TUM/DL4CV/Exercise/dl4cv/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vincent/Documents/TUM/DL4CV/Exercise/dl4cv/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vincent/Documents/TUM/DL4CV/Exercise/dl4cv/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vincent/Documents/TUM/DL4CV/Exercise/dl4cv/.venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_height = 224\n",
    "image_width = 224\n",
    "\n",
    "num_iter = 1000\n",
    "val_every = 100\n",
    "num_val = 10\n",
    "\n",
    "feature_with_bb = {'image/encoded': tf.FixedLenFeature([], tf.string),\n",
    "                   'image/object/bbox/xmin': tf.FixedLenFeature([], tf.float32),\n",
    "                   'image/object/bbox/xmax': tf.FixedLenFeature([], tf.float32),\n",
    "                   'image/object/bbox/ymin': tf.FixedLenFeature([], tf.float32),\n",
    "                   'image/object/bbox/ymax': tf.FixedLenFeature([], tf.float32)}\n",
    "\n",
    "feature_without_bb = {'image/encoded': tf.FixedLenFeature([], tf.string)}\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "with tf.Session(config=config)  as sess:\n",
    "    bb_net = BB_CNN.BB_CNN(kernel_size = 13 * [3], kernel_stride = 13 * [1], num_filters =  2 * [64] + 2 * [128] + 3 * [256] + 6 * [512],\n",
    "                           pool_size = 2 * [1, 2] + 3 * [1, 1, 2], pool_stride = 2 * [1, 2] + 3 * [1, 1, 2], hidden_dim = 2 * [4096],\n",
    "                           dropout = 0.5, weight_scale = 1, file_name = 'vgg16.npy')\n",
    "    #bb_net = BB_CNN.BB_CNN(kernel_size = 7 * [3], kernel_stride = 7 * [1], num_filters =  7 * [4],\n",
    "    #                       pool_size = 7 * [2], pool_stride = 7 * [2], hidden_dim = [100],\n",
    "    #                       dropout = 0.5, weight_scale = 1)\n",
    "    images = tf.placeholder(tf.float32, [1, image_width, image_height, 3])\n",
    "    train_mode = tf.placeholder(tf.bool)\n",
    "    target_prob = tf.placeholder(tf.float32, [1])\n",
    "    target_bb = tf.placeholder(tf.float32, [1, 4])\n",
    "    bb_net.build(images, train_mode)\n",
    "    bb_net.predict()\n",
    "    bb_net.loss(target_prob, target_bb)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(bb_net.loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(iterator_train.initializer)\n",
    "    sess.run(iterator_val.initializer)\n",
    "    for i in range(num_iter):\n",
    "        next_example_train = sess.run(next_element_train)\n",
    "        try:\n",
    "            parsed_out = sess.run(tf.parse_single_example(next_example_train, features=feature_with_bb))\n",
    "            \n",
    "            xmin = max(parsed_out['image/object/bbox/xmin'], 0)\n",
    "            xmax = min(parsed_out['image/object/bbox/xmax'], 1)\n",
    "            ymin = max(parsed_out['image/object/bbox/ymin'], 0)\n",
    "            ymax = min(parsed_out['image/object/bbox/ymax'], 1)\n",
    "            \n",
    "            prob = 1\n",
    "        except:\n",
    "            parsed_out = sess.run(tf.parse_single_example(next_example_train, features=feature_without_bb))\n",
    "            \n",
    "            xmin = 0\n",
    "            xmax = 1\n",
    "            ymin = 0\n",
    "            ymax = 1\n",
    "            \n",
    "            prob = 0\n",
    "            \n",
    "        tf_image = tf.cast(tf.image.decode_jpeg(parsed_out['image/encoded']), tf.float32) / 255.\n",
    "        \n",
    "        # VGG16 only\n",
    "        tf_image = tf.image.resize_images(tf_image, [126, 224])\n",
    "        tf_image = tf.image.resize_image_with_crop_or_pad(tf_image, 224, 224)\n",
    "\n",
    "        image = sess.run(tf_image)\n",
    "        x = image.reshape((1, image_width, image_height, 3))\n",
    "        sess.run(train_step, feed_dict={images: x, train_mode: False, target_prob: [prob], target_bb: [[xmin, ymin, np.log(xmax - xmin), np.log(ymax - ymin)]]})\n",
    "        \n",
    "        if (i + 1) % val_every == 0:\n",
    "            net_loss = np.zeros(num_val)\n",
    "            for j in range(num_val):\n",
    "                next_example_train = sess.run(next_element_train)\n",
    "                try:\n",
    "                    parsed_out = sess.run(tf.parse_single_example(next_example_train, features=feature_with_bb))\n",
    "\n",
    "                    xmin = max(parsed_out['image/object/bbox/xmin'], 0)\n",
    "                    xmax = min(parsed_out['image/object/bbox/xmax'], 1)\n",
    "                    ymin = max(parsed_out['image/object/bbox/ymin'], 0)\n",
    "                    ymax = min(parsed_out['image/object/bbox/ymax'], 1)\n",
    "\n",
    "                    prob = 1\n",
    "                except:\n",
    "                    parsed_out = sess.run(tf.parse_single_example(next_example_train, features=feature_without_bb))\n",
    "\n",
    "                    xmin = 0\n",
    "                    xmax = 1\n",
    "                    ymin = 0\n",
    "                    ymax = 1\n",
    "\n",
    "                    prob = 0\n",
    "                    \n",
    "                tf_image = tf.cast(tf.image.decode_jpeg(parsed_out['image/encoded']), tf.float32) / 255.\n",
    "                \n",
    "                # VGG16 only\n",
    "                tf_image = tf.image.resize_images(tf_image, tf.constant([126, 224]))\n",
    "                tf_image = tf.image.resize_image_with_crop_or_pad(tf_image, 224, 224)\n",
    "                  \n",
    "                image = sess.run(tf_image)\n",
    "                x = image.reshape((1, image_width, image_height, 3))\n",
    "                net_loss[j] = sess.run(bb_net.loss, feed_dict={images: x, train_mode: False, target_prob: [prob], target_bb: [[xmin, ymin, np.log(xmax - xmin), np.log(ymax - ymin)]]})\n",
    "            print('loss after ' + str(i + 1) + ' iterations = ' + str(np.mean(net_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
